{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faae0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/nas/ckgfs/users/minhpham/workspace/seed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08c2378-b776-4a8f-bc5e-95c5cb4631e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TapasForSequenceClassification: ['output_weights', 'aggregation_classifier.bias', 'column_output_bias', 'aggregation_classifier.weight', 'column_output_weights', 'output_bias']\n",
      "- This IS expected if you are initializing TapasForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TapasForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TapasForSequenceClassification were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TapasTokenizer, TapasForSequenceClassification, TapasForQuestionAnswering, TapasConfig\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Load pretrained model: TAPAS finetuned on WikiTable Questions\n",
    "model = TapasForSequenceClassification.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "config = TapasConfig.from_pretrained(\"google/tapas-base-finetuned-tabfact\")\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-tabfact\")\n",
    "\n",
    "model.classifier = nn.Linear(config.hidden_size, 3)\n",
    "model.num_labels = 3\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# model, tokenizer = accelerator.prepare(\n",
    "#     model, tokenizer\n",
    "# )\n",
    "\n",
    "# unwrapped_model = accelerator.unwrap_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2820ce87-771a-4dc5-b69e-1750b1678a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uclean.dataset.infotabs import TableInfotabDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "key2table = {}\n",
    "\n",
    "infotabs_df = pd.read_csv(\"../data/infotab/maindata/infotabs_test_alpha3.tsv\", sep='\\t')\n",
    "\n",
    "for file_name in Path(\"../data/infotab/tables/json\").iterdir():\n",
    "    with open(file_name, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "        obj = {k: v if not isinstance(v, list) else [','.join(v)] for k, v in obj.items()}\n",
    "        key2table[file_name.stem] = pd.DataFrame(obj)\n",
    "\n",
    "data = []\n",
    "for i, row in infotabs_df.iterrows():\n",
    "    example = {}\n",
    "    example[\"table\"] = key2table[row['table_id']]\n",
    "    example[\"label\"] = row['label']\n",
    "    example[\"correct_sentence\"] = row['hypothesis']\n",
    "    data.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f65dfd34",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m----> 5\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m tokenizer(table\u001b[38;5;241m=\u001b[39m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m, queries\u001b[38;5;241m=\u001b[39mexample\u001b[38;5;241m.\u001b[39mcorrect_sentence, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencodings)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      7\u001b[0m     pred_label \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'table'"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "count = 0\n",
    "\n",
    "for example in data:\n",
    "    encodings = tokenizer(table=example[\"table\"], queries=example.correct_sentence, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    logits = model(**encodings).logits\n",
    "    pred_label = logits.argmax(-1).item()\n",
    "    count += 1\n",
    "    if pred_label == (example.label == \"E\"):\n",
    "        correct_count += 1\n",
    "\n",
    "print(f\"Accuracy: {correct_count / len(dataset.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc235d2-0e30-4513-a648-b837025a421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "  \"\"\"\n",
    "    Load\n",
    "  \"\"\"\n",
    "  # Load pretrained tokenizer: TAPAS finetuned on WikiTable Questions\n",
    "  tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "  # Load pretrained model: TAPAS finetuned on WikiTable Questions\n",
    "  model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "  # Return tokenizer and model\n",
    "  return tokenizer, model\n",
    "\n",
    "\n",
    "def prepare_inputs(data, queries, tokenizer):\n",
    "  \"\"\"\n",
    "    Convert dictionary into data frame and tokenize inputs given queries.\n",
    "  \"\"\"\n",
    "  # Prepare inputs\n",
    "  table = pd.DataFrame.from_dict(data)\n",
    "  inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")\n",
    "  \n",
    "  # Return things\n",
    "  return table, inputs\n",
    "\n",
    "\n",
    "def generate_predictions(inputs, model, tokenizer):\n",
    "  \"\"\"\n",
    "    Generate predictions for some tokenized input.\n",
    "  \"\"\"\n",
    "  # Generate model results\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "  # Convert logit outputs into predictions for table cells and aggregation operators\n",
    "  predicted_table_cell_coords, predicted_aggregation_operators = tokenizer.convert_logits_to_predictions(\n",
    "          inputs,\n",
    "          outputs.logits.detach(),\n",
    "          outputs.logits_aggregation.detach()\n",
    "  )\n",
    "  \n",
    "  # Return values\n",
    "  return predicted_table_cell_coords, predicted_aggregation_operators\n",
    "\n",
    "\n",
    "def postprocess_predictions(predicted_aggregation_operators, predicted_table_cell_coords, table):\n",
    "  \"\"\"\n",
    "    Compute the predicted operation and nicely structure the answers.\n",
    "  \"\"\"\n",
    "  # Process predicted aggregation operators\n",
    "  aggregation_operators = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3:\"COUNT\"}\n",
    "  aggregation_predictions_string = [aggregation_operators[x] for x in predicted_aggregation_operators]\n",
    "\n",
    "  # Process predicted table cell coordinates\n",
    "  answers = []\n",
    "  for coordinates in predicted_table_cell_coords:\n",
    "    if len(coordinates) == 1:\n",
    "      # 1 cell\n",
    "      answers.append(table.iat[coordinates[0]])\n",
    "    else:\n",
    "      # > 1 cell\n",
    "      cell_values = []\n",
    "      for coordinate in coordinates:\n",
    "        cell_values.append(table.iat[coordinate])\n",
    "      answers.append(\", \".join(cell_values))\n",
    "      \n",
    "  # Return values\n",
    "  return aggregation_predictions_string, answers\n",
    "\n",
    "\n",
    "def show_answers(queries, answers, aggregation_predictions_string):\n",
    "  \"\"\"\n",
    "    Visualize the postprocessed answers.\n",
    "  \"\"\"\n",
    "  for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n",
    "    print(query)\n",
    "    if predicted_agg == \"NONE\":\n",
    "      print(\"Predicted answer: \" + answer)\n",
    "    else:\n",
    "      print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)\n",
    "\n",
    "\n",
    "def run_tapas():\n",
    "  \"\"\"\n",
    "    Invoke the TAPAS model.\n",
    "  \"\"\"\n",
    "  tokenizer, model = load_model_and_tokenizer()\n",
    "  table, inputs = prepare_inputs(data, queries, tokenizer)\n",
    "  predicted_table_cell_coords, predicted_aggregation_operators = generate_predictions(inputs, model, tokenizer)\n",
    "  aggregation_predictions_string, answers = postprocess_predictions(predicted_aggregation_operators, predicted_table_cell_coords, table)\n",
    "  show_answers(queries, answers, aggregation_predictions_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01eb717d-e451-4b78-b4e2-d4c39be163d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last Asian Beach Games was held in Danang, Vietnam from 24 September to 3 October 2016\n",
      "Predicted answer: Vietnam\n"
     ]
    }
   ],
   "source": [
    "run_tapas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063fec9-b728-4cef-8651-9c718964d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "581f61c2706495f13923e10c4ff7992c8e2b6e3bd501d069320392846270986a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

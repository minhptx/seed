{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = '/nas/ckgfs/users/minhpham/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getenv(\"HF_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "# initialize with RagRetriever to do everything in one forward call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/minhpham/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Using custom data configuration psgs_w100.nq.no_index-dummy=False,with_index=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_dpr/psgs_w100.nq.no_index (download: 66.09 GiB, generated: 73.03 GiB, post-processed: Unknown size, total: 139.13 GiB) to /nas/home/minhpham/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.no_index-dummy=False,with_index=False/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0e02a37f244b21be194e46ea121292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2676f4b5a9544edc87bfb997e8e22257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4995e31b4c54512b0d515e34ba8c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/nas/ckgfs/users/minhpham/workspace/seed/demo/rag.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bckg05.isi.edu/nas/ckgfs/users/minhpham/workspace/seed/demo/rag.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m retriever \u001b[39m=\u001b[39m RagRetriever\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/rag-token-nq\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwiki_dpr\u001b[39;49m\u001b[39m\"\u001b[39;49m, index_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcompressed\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py:430\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[0;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     index \u001b[39m=\u001b[39m CustomHFIndex(config\u001b[39m.\u001b[39mretrieval_vector_size, indexed_dataset)\n\u001b[1;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index(config)\n\u001b[1;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    432\u001b[0m     config,\n\u001b[1;32m    433\u001b[0m     question_encoder_tokenizer\u001b[39m=\u001b[39mquestion_encoder_tokenizer,\n\u001b[1;32m    434\u001b[0m     generator_tokenizer\u001b[39m=\u001b[39mgenerator_tokenizer,\n\u001b[1;32m    435\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py:410\u001b[0m, in \u001b[0;36mRagRetriever._build_index\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[39mreturn\u001b[39;00m CustomHFIndex\u001b[39m.\u001b[39mload_from_disk(\n\u001b[1;32m    405\u001b[0m         vector_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mretrieval_vector_size,\n\u001b[1;32m    406\u001b[0m         dataset_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mpassages_path,\n\u001b[1;32m    407\u001b[0m         index_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mindex_path,\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m CanonicalHFIndex(\n\u001b[1;32m    411\u001b[0m         vector_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mretrieval_vector_size,\n\u001b[1;32m    412\u001b[0m         dataset_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdataset,\n\u001b[1;32m    413\u001b[0m         dataset_split\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdataset_split,\n\u001b[1;32m    414\u001b[0m         index_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mindex_name,\n\u001b[1;32m    415\u001b[0m         index_path\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mindex_path,\n\u001b[1;32m    416\u001b[0m         use_dummy_dataset\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49muse_dummy_dataset,\n\u001b[1;32m    417\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py:265\u001b[0m, in \u001b[0;36mCanonicalHFIndex.__init__\u001b[0;34m(self, vector_size, dataset_name, dataset_split, index_name, index_path, use_dummy_dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_dummy_dataset \u001b[39m=\u001b[39m use_dummy_dataset\n\u001b[1;32m    264\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading passages from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_name, with_index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_split, dummy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_dummy_dataset\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(vector_size, dataset, index_initialized\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/load.py:1679\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1678\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1679\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1680\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1681\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1682\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1683\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1684\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1685\u001b[0m )\n\u001b[1;32m   1687\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1689\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1690\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/builder.py:704\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    705\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager, verify_infos\u001b[39m=\u001b[39;49mverify_infos, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/builder.py:1221\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verify_infos):\n\u001b[0;32m-> 1221\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(dl_manager, verify_infos, check_duplicate_keys\u001b[39m=\u001b[39;49mverify_infos)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/builder.py:771\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    770\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 771\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m    773\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m verify_infos \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54/wiki_dpr.py:130\u001b[0m, in \u001b[0;36mWikiDpr._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m    129\u001b[0m     files_to_download \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdata_file\u001b[39m\u001b[39m\"\u001b[39m: _DATA_URL}\n\u001b[0;32m--> 130\u001b[0m     downloaded_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(files_to_download)\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mwith_embeddings:\n\u001b[1;32m    132\u001b[0m         vectors_url \u001b[39m=\u001b[39m _NQ_VECTORS_URL \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39membeddings_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnq\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m _MULTISET_VECTORS_URL\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/download/download_manager.py:431\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[1;32m    416\u001b[0m     \u001b[39m\"\"\"Download and extract given url_or_urls.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[1;32m    418\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/download/download_manager.py:403\u001b[0m, in \u001b[0;36mDownloadManager.extract\u001b[0;34m(self, path_or_paths, num_proc)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 403\u001b[0m extracted_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    404\u001b[0m     partial(cached_path, download_config\u001b[39m=\u001b[39;49mdownload_config),\n\u001b[1;32m    405\u001b[0m     path_or_paths,\n\u001b[1;32m    406\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    407\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    408\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mExtracting data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m path_or_paths \u001b[39m=\u001b[39m NestedDataStructure(path_or_paths)\n\u001b[1;32m    411\u001b[0m extracted_paths \u001b[39m=\u001b[39m NestedDataStructure(extracted_paths)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/py_utils.py:356\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    354\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m num_proc:\n\u001b[0;32m--> 356\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[1;32m    357\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    358\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    359\u001b[0m     ]\n\u001b[1;32m    360\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     split_kwds \u001b[39m=\u001b[39m []  \u001b[39m# We organize the splits ourselve (contiguous splits)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/py_utils.py:357\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m num_proc:\n\u001b[1;32m    356\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 357\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    358\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    359\u001b[0m     ]\n\u001b[1;32m    360\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     split_kwds \u001b[39m=\u001b[39m []  \u001b[39m# We organize the splits ourselve (contiguous splits)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/py_utils.py:293\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m# Singleton first to spare some computation\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    295\u001b[0m \u001b[39m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m<\u001b[39m logging\u001b[39m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/file_utils.py:213\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mreturn\u001b[39;00m output_path\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mextract_compressed_file:\n\u001b[0;32m--> 213\u001b[0m     output_path \u001b[39m=\u001b[39m ExtractManager(cache_dir\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mcache_dir)\u001b[39m.\u001b[39;49mextract(\n\u001b[1;32m    214\u001b[0m         output_path, force_extract\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_extract\n\u001b[1;32m    215\u001b[0m     )\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m output_path\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/extract.py:41\u001b[0m, in \u001b[0;36mExtractManager.extract\u001b[0;34m(self, input_path, force_extract)\u001b[0m\n\u001b[1;32m     39\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_path(input_path)\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_extract(output_path, force_extract):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextractor\u001b[39m.\u001b[39;49mextract(input_path, output_path, extractor\u001b[39m=\u001b[39;49mextractor)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m output_path\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/extract.py:200\u001b[0m, in \u001b[0;36mExtractor.extract\u001b[0;34m(cls, input_path, output_path, extractor)\u001b[0m\n\u001b[1;32m    198\u001b[0m os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(output_path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m extractor:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m extractor\u001b[39m.\u001b[39;49mextract(input_path, output_path)\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m extractor \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mextractors:\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m extractor\u001b[39m.\u001b[39mis_extractable(input_path):\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/datasets/utils/extract.py:73\u001b[0m, in \u001b[0;36mGzipExtractor.extract\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39mopen(input_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m gzip_file:\n\u001b[1;32m     72\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m extracted_file:\n\u001b[0;32m---> 73\u001b[0m         shutil\u001b[39m.\u001b[39;49mcopyfileobj(gzip_file, extracted_file)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/gzip.py:292\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mread(size)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/gzip.py:487\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    485\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 487\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", dataset=\"wiki_dpr\", index_name=\"compressed\", cache_dir='/nas/ckgfs/users/minhpham/.cache/huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/minhpham/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RagSequenceForGeneration were not initialized from the model checkpoint at facebook/rag-token-nq and are newly initialized: ['rag.generator.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   0,  81, 316, 153,   2,   1,   1,   1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "inputs = tokenizer(\"China\", return_tensors=\"pt\")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "labels = targets[\"input_ids\"]\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s><s> over 12 million</s><pad><pad><pad>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4562e6ed3207b6f9fa47a5acb55a0f6b1fdeb483f0e925c28b3c54e60f016a50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
